{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/tpeat3/.conda/envs/cs7643-a2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# load datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_subset_size = int(0.1 * len(train_dataset))\n",
    "train_subset, val_set = random_split(train_dataset, [train_subset_size, len(train_dataset) - train_subset_size])\n",
    "train_subset_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='../data', train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNN().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# one epoch\n",
    "def train_epoch(model, dataloader, loss_function, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    loss_values = []  # To store loss values for each batch\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Training')\n",
    "    \n",
    "    for i, (inputs, labels) in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "    average_loss = sum(loss_values) / len(loss_values)\n",
    "    print(f\"Epoch {epoch} finished, average loss: {average_loss:.4f}\")\n",
    "    \n",
    "    return running_loss\n",
    "\n",
    "def eval(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Visualize five outputs\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        data_iter = iter(test_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 2))\n",
    "        for idx in range(5):\n",
    "            ax = fig.add_subplot(1, 5, idx + 1, xticks=[], yticks=[])\n",
    "            ax.imshow(images[idx].squeeze().cpu(), cmap='gray')\n",
    "            ax.set_title(f\"Predicted: {predicted[idx].cpu().item()}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
    "    return accuracy, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:04<00:00, 20.59it/s, loss=2.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished, average loss: 2.2656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 75.40it/s, loss=2.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished, average loss: 2.1899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 75.87it/s, loss=2.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished, average loss: 2.1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 76.01it/s, loss=2.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished, average loss: 2.0493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 75.98it/s, loss=2.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished, average loss: 1.9777\n"
     ]
    }
   ],
   "source": [
    "# Training process\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(model, train_subset_loader, loss_fn, optimizer, device, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 51.56%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(51.5625, 124.62004089355469)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(model, test_loader, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet to genMNIST\n",
    "\n",
    "Before we generate weights let genearte MNIST straight up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(1, 64, 2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 2, stride=2, padding=1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(256, 128, 2, stride=2, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 1, 2, stride=2)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.upconv1(x)\n",
    "        x = self.upconv2(x)\n",
    "        x = self.upconv3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:07<00:00, 13.40it/s, loss=0.0676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 55.47it/s, loss=0.0412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 55.58it/s, loss=0.0335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 55.47it/s, loss=0.0299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 55.63it/s, loss=0.0266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0030\n"
     ]
    }
   ],
   "source": [
    "unet = UNet().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_subset_loader), total=len(train_subset_loader), desc='Training')\n",
    "    for i, (inputs, labels) in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        noisy_images = inputs + torch.randn_like(inputs).to(device) * 0.1  # Add noise to the input images\n",
    "        predicted_images = unet(noisy_images)\n",
    "        \n",
    "        # Consistency models\n",
    "        consistency_weight = 0.1\n",
    "        noise_level = 0.2\n",
    "        pred_noise = torch.randn_like(predicted_images).to(device)\n",
    "        noisy_pred_images = predicted_images + pred_noise * noise_level\n",
    "        pred_denoised_images = unet(noisy_pred_images)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        denoising_loss = criterion(predicted_images, inputs)\n",
    "        consistency_loss = criterion(pred_denoised_images, predicted_images)\n",
    "        loss = denoising_loss + consistency_weight * consistency_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "    # Print the average loss for this epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unet.eval()\n",
    "\n",
    "noise = torch.randn(4, 1, 28, 28).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_images = unet(noise)\n",
    "\n",
    "generated_images = generated_images.view(-1, 1, 28, 28)\n",
    "\n",
    "image_grid = make_grid(generated_images, nrow=4, padding=2, normalize=True)\n",
    "\n",
    "image_grid_numpy = image_grid.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# Display the image grid\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_grid_numpy, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we fix this garbage? Add timestep embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "    \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
    "    def __init__(self, embed_dim, scale=30.):\n",
    "        super().__init__()\n",
    "        # Randomly sample weights during initialization. These weights are fixed\n",
    "        # during optimization and are not trainable.\n",
    "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, channel_dims=[32, 64, 128, 256], time_embed_dim=128):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        s = 1\n",
    "        in_dim = 1\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i, out_dim in enumerate(channel_dims):\n",
    "            self.convs.append(nn.Conv2d(in_dim, out_dim, 3, stride=s))\n",
    "            in_dim = out_dim\n",
    "            s = 2\n",
    "        \n",
    "        # Decoder\n",
    "        in_dim = channel_dims[-1]\n",
    "        s = 2\n",
    "        self.upconvs = nn.ModuleList()\n",
    "        for i, out_dim in enumerate(channel_dims[::-1]):\n",
    "            out_pad = 0\n",
    "            if i in [1, 2]:\n",
    "                out_pad = 1\n",
    "            if i == 3:\n",
    "                s = 1\n",
    "            \n",
    "            self.upconvs.append(nn.ConvTranspose2d(in_dim, out_dim, 3, stride=s, output_padding=out_pad))\n",
    "            in_dim = out_dim\n",
    "        \n",
    "        # encoder time embed\n",
    "        self.encoder_dense = nn.ModuleList()\n",
    "        for out_dim in channel_dims:\n",
    "            self.encoder_dense.append(nn.Linear(time_embed_dim, out_dim))\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = GaussianFourierProjection(time_embed_dim)\n",
    "        self.dense = nn.Linear(time_embed_dim, 32)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # Embed time\n",
    "        time_embed = self.time_embed(t)\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x)\n",
    "            if i == 0:\n",
    "                y = self.dense(time_embed)\n",
    "                y = y.unsqueeze(-1).unsqueeze(-1)\n",
    "                x += y\n",
    "            x = self.relu(x)\n",
    "        \n",
    "        # Decoder\n",
    "        for upconv in self.upconvs:\n",
    "            x = upconv(x)\n",
    "        \n",
    "        x = x / marginal_prob_std(t, sigma)[:, None, None, None]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def marginal_prob_std(t, sigma):\n",
    "    \"\"\"\n",
    "    Args:    \n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.  \n",
    "\n",
    "    Returns:\n",
    "    The standard deviation.\n",
    "    \"\"\"    \n",
    "    t = torch.tensor(t, device=device)\n",
    "    return torch.sqrt((sigma**(2 * t) - 1.) / 2. / np.log(sigma))\n",
    "\n",
    "\n",
    "def diffusion_coeff(t, sigma):\n",
    "    \"\"\"Compute the diffusion coefficient of our SDE.\n",
    "\n",
    "    Args:\n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.\n",
    "\n",
    "    Returns:\n",
    "    The vector of diffusion coefficients.\n",
    "    \"\"\"\n",
    "    return torch.tensor(sigma**t, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/94 [00:00<?, ?it/s]/scratch/551306/ipykernel_115452/2131142911.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t, device=device)\n",
      "Training: 100%|██████████| 94/94 [00:01<00:00, 51.73it/s, loss=1.96e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2249.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 51.83it/s, loss=1.74e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1861.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 51.84it/s, loss=1.62e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 1690.7322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 51.85it/s, loss=1.58e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 1593.9285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 51.83it/s, loss=1.48e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 1537.6442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 51.85it/s, loss=1.47e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 1502.2740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 51.85it/s, loss=1.48e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 1472.2758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 51.84it/s, loss=1.42e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 1448.8345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 51.88it/s, loss=1.38e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 1431.6973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:01<00:00, 51.87it/s, loss=1.48e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 1419.1786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = UNet(time_embed_dim=256).to(device)\n",
    "sigma=25\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_subset_loader), total=len(train_subset_loader), desc='Training')\n",
    "    for i, (images, labels) in pbar:\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        time_steps = torch.rand(images.shape[0]).to(device)  # Random time steps between 0 and 1\n",
    "        \n",
    "        # pass in noisy image and time step\n",
    "        pred_noise = torch.randn_like(images).to(device)\n",
    "        noise_level = marginal_prob_std(time_steps, sigma)\n",
    "        noisy_pred_images = images + pred_noise * noise_level[:, None, None, None]\n",
    "        score = model2(noisy_pred_images, time_steps)\n",
    "        \n",
    "        # loss taken from consistency models\n",
    "        loss = torch.mean(torch.sum((score * noise_level[:, None, None, None] + pred_noise)**2, dim=(1,2,3)))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/551306/ipykernel_115452/2131142911.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t, device=device)\n",
      "/scratch/551306/ipykernel_115452/2131142911.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sigma**t, device=device)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2.eval()\n",
    "\n",
    "bs = 4\n",
    "\n",
    "time = torch.ones(bs).to(device)\n",
    "noise = torch.randn(bs, 1, 28, 28).to(device) * marginal_prob_std(time, sigma)[:, None, None, None]\n",
    "\n",
    "step_size = torch.tensor(10).to(device)\n",
    "for step in range(0, 100, 10):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bs_time = time * step\n",
    "        g = diffusion_coeff(bs_time, sigma)\n",
    "        generated_images = model2(noise, bs_time)\n",
    "        print(generated_images.shape)\n",
    "#         mean_noise = noise + (g**2)[:, None, None, None] * generated_images * step_size\n",
    "        # noise = mean_noise + g[:, None, None, None] * torch.randn_like(noise).to(device)\n",
    "        \n",
    "    # generated_images = mean_noise.clamp(0, 1)\n",
    "    generated_images = generated_images.view(-1, 1, 28, 28)\n",
    "    print(generated_images.shape)\n",
    "\n",
    "    image_grid = make_grid(generated_images, nrow=4, padding=2, normalize=True)\n",
    "\n",
    "    image_grid_numpy = image_grid.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Display the image grid\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image_grid_numpy, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closer; something wants to happen, just gotta push it there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/550893/ipykernel_46240/885697660.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t, device=device)\n",
      "/scratch/550893/ipykernel_46240/108517298.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample = torch.tensor(sample, device=device, dtype=dtype).reshape(shape)\n",
      "/scratch/550893/ipykernel_46240/108517298.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  time_steps = torch.tensor(time_steps, device=device, dtype=dtype).reshape((sample.shape[0], ))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100352,) (3136,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (g\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m score\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Run the black-box ODE solver.\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mintegrate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve_ivp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mode_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRK45\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of function evaluations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mnfev\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(res\u001b[38;5;241m.\u001b[39my[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mreshape(shape)\n",
      "File \u001b[0;32m~/.conda/envs/cs7643-a2/lib/python3.9/site-packages/scipy/integrate/_ivp/ivp.py:616\u001b[0m, in \u001b[0;36msolve_ivp\u001b[0;34m(fun, t_span, y0, method, t_eval, dense_output, events, vectorized, args, **options)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m METHODS:\n\u001b[1;32m    614\u001b[0m     method \u001b[38;5;241m=\u001b[39m METHODS[method]\n\u001b[0;32m--> 616\u001b[0m solver \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t_eval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     ts \u001b[38;5;241m=\u001b[39m [t0]\n",
      "File \u001b[0;32m~/.conda/envs/cs7643-a2/lib/python3.9/site-packages/scipy/integrate/_ivp/rk.py:96\u001b[0m, in \u001b[0;36mRungeKutta.__init__\u001b[0;34m(self, fun, t0, y0, t_bound, max_step, rtol, atol, vectorized, first_step, **extraneous)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_abs \u001b[38;5;241m=\u001b[39m \u001b[43mselect_initial_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_estimator_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_abs \u001b[38;5;241m=\u001b[39m validate_first_step(first_step, t0, t_bound)\n",
      "File \u001b[0;32m~/.conda/envs/cs7643-a2/lib/python3.9/site-packages/scipy/integrate/_ivp/common.py:108\u001b[0m, in \u001b[0;36mselect_initial_step\u001b[0;34m(fun, t0, y0, f0, direction, order, rtol, atol)\u001b[0m\n\u001b[1;32m    106\u001b[0m scale \u001b[38;5;241m=\u001b[39m atol \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(y0) \u001b[38;5;241m*\u001b[39m rtol\n\u001b[1;32m    107\u001b[0m d0 \u001b[38;5;241m=\u001b[39m norm(y0 \u001b[38;5;241m/\u001b[39m scale)\n\u001b[0;32m--> 108\u001b[0m d1 \u001b[38;5;241m=\u001b[39m norm(\u001b[43mf0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m d0 \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-5\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m d1 \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-5\u001b[39m:\n\u001b[1;32m    110\u001b[0m     h0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100352,) (3136,) "
     ]
    }
   ],
   "source": [
    "atol=1e-4\n",
    "rtol=1e-5\n",
    "z = None\n",
    "eps=1e-6\n",
    "dtype=torch.float32\n",
    "noise = torch.randn(bs, 1, 28, 28).to(device) * marginal_prob_std(time, sigma)[:, None, None, None]\n",
    "init_x = noise\n",
    "shape = init_x.shape\n",
    "init_x = torch.clamp(init_x, min=-1e5, max=1e5).to(torch.float32)\n",
    "t = torch.ones(batch_size, device=device)\n",
    "\n",
    "shape = init_x.shape\n",
    "intermediate_samples = []\n",
    "visualize=True\n",
    "\n",
    "def score_eval_wrapper(sample, time_steps):\n",
    "    \"\"\"A wrapper of the score-based model for use by the ODE solver.\"\"\"\n",
    "    sample = torch.tensor(sample, device=device, dtype=dtype).reshape(shape)\n",
    "    time_steps = torch.tensor(time_steps, device=device, dtype=dtype).reshape((sample.shape[0], ))\n",
    "    with torch.no_grad():\n",
    "        score = model2(sample, time_steps)\n",
    "    return score.cpu().numpy().reshape((-1,)).astype(np.float32)\n",
    "\n",
    "def ode_func(t, x):\n",
    "    \"\"\"The ODE function for use by the ODE solver.\"\"\"\n",
    "    if visualize:\n",
    "        intermediate_samples.append(torch.tensor(x, device=device, dtype=dtype).reshape(shape))\n",
    "    time_steps = torch.ones((shape[0],), device=device) * t\n",
    "    g = diffusion_coeff(t, sigma).cpu().numpy()\n",
    "    score = score_eval_wrapper(torch.tensor(x, device=device).reshape(shape), time_steps)\n",
    "    return -0.5 * (g**2) * score\n",
    "\n",
    "# Run the black-box ODE solver.\n",
    "res = integrate.solve_ivp(ode_func, (1., eps), init_x.reshape(-1).cpu().numpy(), rtol=rtol, atol=atol, method='RK45')\n",
    "print(f\"Number of function evaluations: {res.nfev}\")\n",
    "\n",
    "x = torch.tensor(res.y[:, -1], device=device).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 28, 28])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_samples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intermediate(intermediate_samples, device='cuda', dtype=torch.float32):\n",
    "    # 8 images to visualize\n",
    "    num_visualization_timesteps = 8\n",
    "    # selects evenly spaces images\n",
    "    visualization_indices = np.linspace(0, len(intermediate_samples) - 1, num_visualization_timesteps, dtype=int)\n",
    "    visualization_samples = [intermediate_samples[i] for i in visualization_indices]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 8, figsize=(20, 2.5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        sample = torch.tensor(visualization_samples[i], device=device, dtype=dtype)\n",
    "        sample = sample.clamp(0.0, 1.0)\n",
    "        # necessary transform for cifar10\n",
    "        sample = sample[0].permute(2, 1, 0).squeeze(0)\n",
    "        # print(sample.shape)\n",
    "        ax.imshow(sample.cpu().numpy(), vmin=0., vmax=1., cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/550893/ipykernel_46240/972691376.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample = torch.tensor(visualization_samples[i], device=device, dtype=dtype)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x250 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_intermediate(intermediate_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_images = x.clamp(0, 1)\n",
    "generated_images = generated_images.view(-1, 1, 28, 28)\n",
    "\n",
    "image_grid = make_grid(generated_images, nrow=4, padding=2, normalize=True)\n",
    "\n",
    "image_grid_numpy = image_grid.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# Display the image grid\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_grid_numpy, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'UNet' object has no attribute 'fc1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UNet' object has no attribute 'fc1'"
     ]
    }
   ],
   "source": [
    "model.fc1.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=512, kernel_size=3, padding=1):\n",
    "        super(SimpleUNetGenerator, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size, padding=padding)\n",
    "\n",
    "        # Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(64, 32, kernel_size, stride=2, padding=padding, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(32, out_channels, kernel_size, stride=2, padding=padding, output_padding=1)\n",
    "\n",
    "        # Final layer\n",
    "        self.final = nn.Linear(out_channels * 7 * 7, 512 * 784)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "\n",
    "        # Decoder\n",
    "        x = nn.functional.relu(self.deconv1(x))\n",
    "        x = nn.functional.relu(self.deconv2(x))\n",
    "\n",
    "        # Final layer\n",
    "        x = x.view(-1, self.out_channels * 7 * 7)\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fc_unet = SimpleUNetGenerator().to(device)\n",
    "noise = torch.randn(1, 1, 28, 28).to(device)\n",
    "weight_matrices = fc_unet(noise)\n",
    "weight_matrices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels=64):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(1, num_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels * 2, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels * 2)\n",
    "        self.conv3 = nn.Conv2d(num_channels * 2, num_channels * 4, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_channels * 4)\n",
    "\n",
    "        # Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(num_channels * 4, num_channels * 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_channels * 2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(num_channels * 2, num_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(num_channels)\n",
    "        self.deconv3 = nn.ConvTranspose2d(num_channels, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        # Decoder\n",
    "        x = F.relu(self.bn4(self.deconv1(x)))\n",
    "        x = F.relu(self.bn5(self.deconv2(x)))\n",
    "        x = torch.sigmoid(self.deconv3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weights_from_unet(two_layer_nn, unet_model, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        weight_tensors = unet_model(input_tensor)\n",
    "\n",
    "        # Flatten the UNet output to a 1D tensor\n",
    "        weight_tensors = weight_tensors.view(-1)\n",
    "        total_elements_needed = 512*28*28 + 512 + 512*10 + 10\n",
    "\n",
    "        # Check if the UNet output has the correct total number of elements\n",
    "        assert weight_tensors.numel() == total_elements_needed, f\"UNet output has {weight_tensors.numel()} elements but {total_elements_needed} elements are needed.\"\n",
    "\n",
    "        # Split the tensor into the respective shapes for fc1 weights, fc1 biases, fc2 weights, and fc2 biases\n",
    "        fc1_weight, fc1_bias, fc2_weight, fc2_bias = torch.split(weight_tensors, [512*28*28, 512, 512*10, 10])\n",
    "\n",
    "        # Reshape and assign the weights and biases to TwoLayerNN\n",
    "        two_layer_nn.fc1.weight = nn.Parameter(fc1_weight.reshape(512, 28*28))\n",
    "        two_layer_nn.fc1.bias = nn.Parameter(fc1_bias.reshape(512))\n",
    "        two_layer_nn.fc2.weight = nn.Parameter(fc2_weight.reshape(10, 512))\n",
    "        two_layer_nn.fc2.bias = nn.Parameter(fc2_bias.reshape(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (1, 1, 28, 28)  # Input shape for the U-Net generator\n",
    "output_size = (512 * 28 * 28)  # Output shape for the weight matrices\n",
    "\n",
    "unet_generator = UNetGenerator(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = torch.randn(1, 1, 28, 28)\n",
    "weight_matrices = unet_generator(noise)\n",
    "weight_matrices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[512, 784]' is invalid for input of size 784",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m fc2_bias_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(fc2_weight_indices\u001b[38;5;241m.\u001b[39mstop, fc2_weight_indices\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m+\u001b[39m fc2_bias_size)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create the state dictionary\u001b[39;00m\n\u001b[1;32m     21\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfc1.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mweight_matrices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfc1_weight_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfc1.bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: weight_matrices[fc1_bias_indices]\u001b[38;5;241m.\u001b[39mview(model\u001b[38;5;241m.\u001b[39mfc1\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mshape),\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfc2.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: weight_matrices[fc2_weight_indices]\u001b[38;5;241m.\u001b[39mview(model\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape),\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfc2.bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: weight_matrices[fc2_bias_indices]\u001b[38;5;241m.\u001b[39mview(model\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mshape),\n\u001b[1;32m     26\u001b[0m }\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[512, 784]' is invalid for input of size 784"
     ]
    }
   ],
   "source": [
    "noise = torch.randn(1, 1, 28, 28)\n",
    "weight_matrices = unet_generator(noise)\n",
    "weight_matrices = weight_matrices.view(-1)\n",
    "\n",
    "# Load the generated weight matrices into the TwoLayerNN model\n",
    "model = TwoLayerNN()\n",
    "\n",
    "# Get the actual sizes of the weight and bias tensors\n",
    "fc1_weight_size = model.fc1.weight.numel()\n",
    "fc1_bias_size = model.fc1.bias.numel()\n",
    "fc2_weight_size = model.fc2.weight.numel()\n",
    "fc2_bias_size = model.fc2.bias.numel()\n",
    "\n",
    "# Calculate the slicing indices dynamically\n",
    "fc1_weight_indices = slice(0, fc1_weight_size)\n",
    "fc1_bias_indices = slice(fc1_weight_indices.stop, fc1_weight_indices.stop + fc1_bias_size)\n",
    "fc2_weight_indices = slice(fc1_bias_indices.stop, fc1_bias_indices.stop + fc2_weight_size)\n",
    "fc2_bias_indices = slice(fc2_weight_indices.stop, fc2_weight_indices.stop + fc2_bias_size)\n",
    "\n",
    "# Create the state dictionary\n",
    "state_dict = {\n",
    "    \"fc1.weight\": weight_matrices[fc1_weight_indices].view(model.fc1.weight.shape),\n",
    "    \"fc1.bias\": weight_matrices[fc1_bias_indices].view(model.fc1.bias.shape),\n",
    "    \"fc2.weight\": weight_matrices[fc2_weight_indices].view(model.fc2.weight.shape),\n",
    "    \"fc2.bias\": weight_matrices[fc2_bias_indices].view(model.fc2.bias.shape),\n",
    "}\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet()\n",
    "unet_optimizer = torch.optim.Adam(unet.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 1, 1, 28, 28]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m unet_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# input_tensor = images.unsqueeze(1)  # Adjust input tensor shape if necessary for UNet\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mapply_weights_from_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtwo_layer_nn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Compute the prediction loss with the current weights\u001b[39;00m\n\u001b[1;32m     11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m two_layer_nn(images)\n",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m, in \u001b[0;36mapply_weights_from_unet\u001b[0;34m(two_layer_nn, unet_model, input_tensor)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_weights_from_unet\u001b[39m(two_layer_nn, unet_model, input_tensor):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m         weight_tensors \u001b[38;5;241m=\u001b[39m \u001b[43munet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(weight_tensors\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# Calculate the total number of elements required for each layer\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Apply down-sampling layers\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown2(x)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Apply up-sampling layers\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 1, 1, 28, 28]"
     ]
    }
   ],
   "source": [
    "## training unet\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        # Generate weights and biases for TwoLayerNN using UNet\n",
    "        unet_optimizer.zero_grad()\n",
    "        # input_tensor = images.unsqueeze(1)  # Adjust input tensor shape if necessary for UNet\n",
    "        apply_weights_from_unet(two_layer_nn, unet, input_tensor)\n",
    "\n",
    "        # Compute the prediction loss with the current weights\n",
    "        predictions = two_layer_nn(images)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(predictions, labels)\n",
    "\n",
    "        # Backpropagate the loss through TwoLayerNN and UNet\n",
    "        loss.backward()\n",
    "        unet_optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "cs7643-a2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
